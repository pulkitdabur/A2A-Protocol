{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -qqqq langgraph uv databricks-agents mlflow-skinny[databricks] databricks-mcp databricks-langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78856b5f",
   "metadata": {},
   "source": [
    "# Langgraph Agent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "import asyncio\n",
    "from typing import Annotated, Any, Generator, List, Optional, Sequence, TypedDict, Union\n",
    "\n",
    "import mlflow\n",
    "import nest_asyncio\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    VectorSearchRetrieverTool,\n",
    ")\n",
    "from databricks_mcp import DatabricksMCPClient, DatabricksOAuthClientProvider\n",
    "from langchain.messages import AIMessage, AIMessageChunk, AnyMessage\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client as connect\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "from pydantic import create_model\n",
    "import os\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "LLM_ENDPOINT_NAME = os.environ[\"LLM_NAME\"]\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "# TODO: Update with your system prompt\n",
    "system_prompt = \"You are a helpful assistant that can run Python code.\"\n",
    "\n",
    "workspace_client = WorkspaceClient()\n",
    "\n",
    "host = workspace_client.config.host\n",
    "# MANAGED_MCP_SERVER_URLS = [\n",
    "#     f\"{host}/api/2.0/mcp/functions/system/ai\",  # Default managed MCP endpoint\n",
    "#     # Example for external MCP:\n",
    "#     # \"https://<workspace-hostname>/api/2.0/mcp/external/{connection_name}\"\n",
    "# ]\n",
    "host=\"<DATABRICKS_WORKSPACE_URL>\"\n",
    "workspace_client = WorkspaceClient(\n",
    "    host=host,\n",
    "    client_id=os.getenv(\"DATABRICKS_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"DATABRICKS_CLIENT_SECRET\"),\n",
    ")\n",
    "\n",
    "#####################\n",
    "## MCP Tool Creation\n",
    "#####################\n",
    "\n",
    "# Define a custom LangChain tool that wraps functionality for calling MCP servers\n",
    "class MCPTool(BaseTool):\n",
    "    \"\"\"Custom LangChain tool that wraps MCP server functionality\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        description: str,\n",
    "        args_schema: type,\n",
    "        server_url: str,\n",
    "        ws: WorkspaceClient,\n",
    "        is_custom: bool = False,\n",
    "    ):\n",
    "        # Initialize the tool\n",
    "        super().__init__(name=name, description=description, args_schema=args_schema)\n",
    "        # Store custom attributes: MCP server URL, Databricks workspace client, and whether the tool is for a custom server\n",
    "        object.__setattr__(self, \"server_url\", server_url)\n",
    "        object.__setattr__(self, \"workspace_client\", ws)\n",
    "        object.__setattr__(self, \"is_custom\", is_custom)\n",
    "\n",
    "    def _run(self, **kwargs) -> str:\n",
    "        \"\"\"Execute the MCP tool\"\"\"\n",
    "        if self.is_custom:\n",
    "            # Use the async method for custom MCP servers (OAuth required)\n",
    "            return asyncio.run(self._run_custom_async(**kwargs))\n",
    "        else:\n",
    "            # Use managed MCP server via synchronous call\n",
    "            mcp_client = DatabricksMCPClient(\n",
    "                server_url=self.server_url, workspace_client=self.workspace_client\n",
    "            )\n",
    "            response = mcp_client.call_tool(self.name, kwargs)\n",
    "            return \"\".join([c.text for c in response.content])\n",
    "\n",
    "    async def _run_custom_async(self, **kwargs) -> str:\n",
    "        \"\"\"Execute custom MCP tool asynchronously\"\"\"\n",
    "        async with connect(\n",
    "            self.server_url, auth=DatabricksOAuthClientProvider(self.workspace_client)\n",
    "        ) as (\n",
    "            read_stream,\n",
    "            write_stream,\n",
    "            _,\n",
    "        ):\n",
    "            # Create an async session with the server and call the tool\n",
    "            async with ClientSession(read_stream, write_stream) as session:\n",
    "                await session.initialize()\n",
    "                response = await session.call_tool(self.name, kwargs)\n",
    "                return \"\".join([c.text for c in response.content])\n",
    "\n",
    "\n",
    "# Retrieve tool definitions from a custom MCP server (OAuth required)\n",
    "async def get_custom_mcp_tools(ws: WorkspaceClient, server_url: str):\n",
    "    \"\"\"Get tools from a custom MCP server using OAuth\"\"\"\n",
    "    async with connect(server_url, auth=DatabricksOAuthClientProvider(ws)) as (\n",
    "        read_stream,\n",
    "        write_stream,\n",
    "        _,\n",
    "    ):\n",
    "        async with ClientSession(read_stream, write_stream) as session:\n",
    "            await session.initialize()\n",
    "            tools_response = await session.list_tools()\n",
    "            return tools_response.tools\n",
    "\n",
    "\n",
    "# Retrieve tool definitions from a managed MCP server\n",
    "def get_managed_mcp_tools(ws: WorkspaceClient, server_url: str):\n",
    "    \"\"\"Get tools from a managed MCP server\"\"\"\n",
    "    mcp_client = DatabricksMCPClient(server_url=server_url, workspace_client=ws)\n",
    "    return mcp_client.list_tools()\n",
    "\n",
    "\n",
    "# Convert an MCP tool definition into a LangChain-compatible tool\n",
    "def create_langchain_tool_from_mcp(\n",
    "    mcp_tool, server_url: str, ws: WorkspaceClient, is_custom: bool = False\n",
    "):\n",
    "    \"\"\"Create a LangChain tool from an MCP tool definition\"\"\"\n",
    "    schema = mcp_tool.inputSchema.copy()\n",
    "    properties = schema.get(\"properties\", {})\n",
    "    required = schema.get(\"required\", [])\n",
    "\n",
    "    # Map JSON schema types to Python types for input validation\n",
    "    TYPE_MAPPING = {\"integer\": int, \"number\": float, \"boolean\": bool}\n",
    "    field_definitions = {}\n",
    "    for field_name, field_info in properties.items():\n",
    "        field_type_str = field_info.get(\"type\", \"string\")\n",
    "        field_type = TYPE_MAPPING.get(field_type_str, str)\n",
    "\n",
    "        if field_name in required:\n",
    "            field_definitions[field_name] = (field_type, ...)\n",
    "        else:\n",
    "            field_definitions[field_name] = (field_type, None)\n",
    "\n",
    "    # Dynamically create a Pydantic schema for the tool's input arguments\n",
    "    args_schema = create_model(f\"{mcp_tool.name}Args\", **field_definitions)\n",
    "\n",
    "    # Return a configured MCPTool instance\n",
    "    return MCPTool(\n",
    "        name=mcp_tool.name,\n",
    "        description=mcp_tool.description or f\"Tool: {mcp_tool.name}\",\n",
    "        args_schema=args_schema,\n",
    "        server_url=server_url,\n",
    "        ws=ws,\n",
    "        is_custom=is_custom,\n",
    "    )\n",
    "\n",
    "\n",
    "# Gather all tools from managed and custom MCP servers into a single list\n",
    "async def create_mcp_tools(\n",
    "    ws: WorkspaceClient, managed_server_urls: List[str] = None, custom_server_urls: List[str] = None\n",
    ") -> List[MCPTool]:\n",
    "    \"\"\"Create LangChain tools from both managed and custom MCP servers\"\"\"\n",
    "    tools = []\n",
    "\n",
    "    if managed_server_urls:\n",
    "        # Load managed MCP tools\n",
    "        for server_url in managed_server_urls:\n",
    "            try:\n",
    "                mcp_tools = get_managed_mcp_tools(ws, server_url)\n",
    "                for mcp_tool in mcp_tools:\n",
    "                    tool = create_langchain_tool_from_mcp(mcp_tool, server_url, ws, is_custom=False)\n",
    "                    tools.append(tool)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading tools from managed server {server_url}: {e}\")\n",
    "\n",
    "    if custom_server_urls:\n",
    "        # Load custom MCP tools (async)\n",
    "        for server_url in custom_server_urls:\n",
    "            try:\n",
    "                mcp_tools = await get_custom_mcp_tools(ws, server_url)\n",
    "                for mcp_tool in mcp_tools:\n",
    "                    tool = create_langchain_tool_from_mcp(mcp_tool, server_url, ws, is_custom=True)\n",
    "                    tools.append(tool)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading tools from custom server {server_url}: {e}\")\n",
    "\n",
    "    return tools\n",
    "\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "\n",
    "# The state for the agent workflow, including the conversation and any custom data\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], add_messages]\n",
    "    custom_inputs: Optional[dict[str, Any]]\n",
    "    custom_outputs: Optional[dict[str, Any]]\n",
    "\n",
    "\n",
    "# Define the LangGraph agent that can call tools\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    "):\n",
    "    model = model.bind_tools(tools)  # Bind tools to the model\n",
    "\n",
    "    # Function to check if agent should continue or finish based on last message\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If function (tool) calls are present, continue; otherwise, end\n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    # Preprocess: optionally prepend a system prompt to the conversation history\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "\n",
    "    model_runnable = preprocessor | model  # Chain the preprocessor and the model\n",
    "\n",
    "    # The function to invoke the model within the workflow\n",
    "    def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)  # Create the agent's state machine\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))  # Agent node (LLM)\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))  # Tools node\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")  # Start at agent node\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",  # If the model requests a tool call, move to tools node\n",
    "            \"end\": END,  # Otherwise, end the workflow\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")  # After tools are called, return to agent node\n",
    "\n",
    "    # Compile and return the tool-calling agent workflow\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "# ResponsesAgent class to wrap the compiled agent and make it compatible with Mosaic AI Responses API\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    # Make a prediction (single-step) for the agent\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\" or event.type == \"error\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    # Stream predictions for the agent, yielding output as it's generated\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        cc_msgs = to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        # Stream events from the agent graph\n",
    "        for event in self.agent.stream({\"messages\": cc_msgs}, stream_mode=[\"updates\", \"messages\"]):\n",
    "            if event[0] == \"updates\":\n",
    "                # Stream updated messages from the workflow nodes\n",
    "                for node_data in event[1].values():\n",
    "                    if len(node_data.get(\"messages\", [])) > 0:\n",
    "                        yield from output_to_responses_items_stream(node_data[\"messages\"])\n",
    "            elif event[0] == \"messages\":\n",
    "                # Stream generated text message chunks\n",
    "                try:\n",
    "                    chunk = event[1][0]\n",
    "                    if isinstance(chunk, AIMessageChunk) and (content := chunk.content):\n",
    "                        yield ResponsesAgentStreamEvent(\n",
    "                            **self.create_text_delta(delta=content, item_id=chunk.id),\n",
    "                        )\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "def str_to_list(server_urls: str) -> list:\n",
    "    return [url.strip() for url in server_urls.split(\",\") if url.strip()]\n",
    "\n",
    "\n",
    "# Initialize the entire agent, including MCP tools and workflow\n",
    "def initialize_agent():\n",
    "    \"\"\"Initialize the agent with MCP tools\"\"\"\n",
    "    # Create MCP tools from the configured servers\n",
    "\n",
    "    MANAGED_MCP_SERVER_URLS = str_to_list(os.getenv[\"MANAGED_MCP_SERVER_URLS\"])\n",
    "    CUSTOM_MCP_SERVER_URLS = str_to_list(os.getenv[\"CUSTOM_MCP_SERVER_URLS\"])\n",
    "    mcp_tools = asyncio.run(\n",
    "        create_mcp_tools(\n",
    "            ws=workspace_client,\n",
    "            managed_server_urls=MANAGED_MCP_SERVER_URLS,\n",
    "            custom_server_urls=CUSTOM_MCP_SERVER_URLS,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the agent graph with an LLM, tool set, and system prompt (if given)\n",
    "    agent = create_tool_calling_agent(llm, mcp_tools, system_prompt)\n",
    "    return LangGraphResponsesAgent(agent)\n",
    "\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = initialize_agent()\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c70aac",
   "metadata": {},
   "source": [
    "# Test your agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b59dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_str(server_urls: list) -> str:\n",
    "    return \",\".join(server_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "workspace_client = WorkspaceClient()\n",
    "dbutils = workspace_client.dbutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df77c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import (\n",
    "    LLM_NAME,\n",
    "    MANAGED_MCP_SERVER_URLS,\n",
    "    CUSTOM_MCP_SERVER_URLS,\n",
    "    SP_SCOPE_NAME,\n",
    "    SP_CLIENT_ID,\n",
    "    SP_CLIENT_SECRE,\n",
    ")\n",
    "\n",
    "os.environ[\"DATABRICKS_CLIENT_ID\"] = dbutils.secrets.get(\n",
    "    scope=SP_SCOPE_NAME, key=SP_CLIENT_ID\n",
    ")\n",
    "os.environ[\"DATABRICKS_CLIENT_SECRET\"] = dbutils.secrets.get(\n",
    "    scope=SP_SCOPE_NAME, key=SP_CLIENT_SECRE\n",
    ")\n",
    "os.environ[\"LLM_NAME\"] = LLM_NAME\n",
    "os.environ[\"MANAGED_MCP_SERVER_URLS\"] = list_to_str(MANAGED_MCP_SERVER_URLS)\n",
    "os.environ[\"CUSTOM_MCP_SERVER_URLS\"] = list_to_str(CUSTOM_MCP_SERVER_URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7b0b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "query = \"<enter your query here>\"\n",
    "AGENT.predict({\"input\": [{\"role\": \"user\", \"content\": query}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0546e1e",
   "metadata": {},
   "source": [
    "# Log the agent as an MLflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.resources import DatabricksServingEndpoint, DatabricksFunction\n",
    "from pkg_resources import get_distribution\n",
    "from config import MLFLOW_NAME, SCHEMA_NAME, MODEL_NAME, CATALOG_NAME\n",
    "\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_NAME),\n",
    "    DatabricksFunction(function_name=\"system.ai.python_exec\"),\n",
    "]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=MLFLOW_NAME,\n",
    "        python_model=\"agent.py\",\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            \"databricks-mcp\",\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "            f\"mcp=={get_distribution('mcp').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f02573e",
   "metadata": {},
   "source": [
    "# Register the model to Unity Catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = CATALOG_NAME\n",
    "schema = SCHEMA_NAME\n",
    "model_name = MODEL_NAME\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80753a",
   "metadata": {},
   "source": [
    "# Deploy the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0eb299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "agents.deploy(\n",
    "    UC_MODEL_NAME,\n",
    "    uc_registered_model_info.version,\n",
    "    environment_vars={\n",
    "        \"DATABRICKS_CLIENT_ID\": f\"{{{{secrets/{SP_SCOPE_NAME}/{SP_CLIENT_ID}}}}}\",\n",
    "        \"DATABRICKS_CLIENT_SECRET\": f\"{{{{secrets/{SP_SCOPE_NAME}/{SP_CLIENT_SECRE}}}}}\",\n",
    "        \"LLM_NAME\": f\"{LLM_NAME}\",\n",
    "        \"MANAGED_MCP_SERVER_URLS\": f\"{list_to_str(MANAGED_MCP_SERVER_URLS)}\",\n",
    "        \"CUSTOM_MCP_SERVER_URLS\": f\"{list_to_str(CUSTOM_MCP_SERVER_URLS)}\",\n",
    "    },\n",
    "    tags={\"endpointSource\": \"docs\"},\n",
    "    deploy_feedback_model=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
